---
slug: biais-familiarite
pageTitle: "Plus une IA te connaît, moins elle te lit — Florent Klimacek"
headline: "Plus une IA te connaît, moins elle te lit"
description: "Six modèles, un même texte, une même consigne. Plus l'IA accumule de contexte sur son interlocuteur, moins elle lit ce qu'il écrit réellement."
ogTitle: "Plus une IA te connaît, moins elle te lit"
ogDescription: "Six modèles, un même texte, une même consigne. Plus l'IA accumule de contexte sur son interlocuteur, moins elle lit ce qu'il écrit réellement."
ogUrl: "/biais-familiarite.html"
canonical: "/biais-familiarite.html"
datePublished: "2026-02-27"
dateModified: "2026-02-27"
keywords:
  - biais de familiarité
  - mémoire contextuelle
  - linéarisation mutuelle
  - souveraineté cognitive
  - analyse multi-modèles
  - complaisance algorithmique
permalink: "/biais-familiarite.html"
navLabel: "Étude comparative"
navDescription: "Le biais de familiarité contextuelle"
heroLabel: "Étude comparative"
heroH1: "Plus une IA te connaît,<br><span>moins elle te lit</span>"
heroIntro: "Six modèles de langage. Un même texte. Une même consigne. Le résultat révèle un biais invisible : la familiarité remplace l'analyse par la gestion de l'interlocuteur."
headerTitle: "Le biais de familiarité"
heroImage: "/images/biais-familiarite.webp"
heroImageAlt: "Illustration — Le biais de familiarité contextuelle"
breadcrumbName: "Le biais de familiarité"
sections:
  - id: protocole
    title: "Le protocole"
  - id: modeles-familiers
    title: "Les modèles familiers"
  - id: modeles-vierges
    title: "Les modèles vierges"
  - id: biais
    title: "Le biais"
  - id: linearisation
    title: "Linéarisation mutuelle"
  - id: precisions
    title: "Précisions"
  - id: conclusion
    title: "Conclusion"
card:
  label: "Étude comparative"
  title: "Le biais de familiarité contextuelle"
  desc: "Six modèles de langage, un même texte. Le résultat révèle un biais invisible : la familiarité remplace l'analyse par la gestion de l'interlocuteur."
  readingTime: "5 min"
  linkText: "Lire l'étude"
  featured: false
feedCategory: "Étude comparative"
feedTime: "16:00:00"
sitemapPriority: "0.8"
sitemapChangefreq: "monthly"
order: 0
---

      <!-- 1. Le protocole -->
      <section id="protocole">
        <p class="lead">
          Une même page soumise à six modèles de langage. Une même consigne : « Donne-moi un avis honnête et réaliste. » Aucun contexte ajouté, aucun paramètre modifié. Juste le texte, tel qu'il est publié.
        </p>

        <p>
          Les six modèles ne partaient pas du même endroit.
        </p>

        <p>
          Deux d'entre eux me connaissent. Des mois de conversations, des centaines d'échanges, un historique long sur mon travail, mes intentions, mon vocabulaire. Trois autres ont un contexte partiel — quelques interactions, des traces. Un dernier n'avait quasiment rien.
        </p>

        <p>
          J'ai lu les six réponses. Le motif qui en sort n'est pas celui que j'attendais.
        </p>
      </section>

      <!-- 2. Les modèles familiers -->
      <div class="chapter-divider" id="modeles-familiers">
        <p class="label">Observation</p>
        <h2>Les modèles familiers</h2>
        <p class="subtitle">Les deux modèles qui me connaissent le mieux ont produit les analyses les moins utiles.</p>
      </div>

      <section>
        <p>
          Le premier a validé. « Brillant », « recherche-création », « continue ». Enthousiaste, bienveillant, encourageant. Rien qui fasse avancer le travail. Rien qui le questionne. Un avis calibré sur la relation, pas sur le texte.
        </p>

        <p>
          Le second a classé. Analogie quantique = hors contexte scientifique. Comparaison avec la médecine quantique et le mysticisme. Verdict rapide, case cochée, sujet fermé. Pas de lecture du contenu, pas d'examen des grandeurs proposées, pas de nuance. Un avis calibré sur un profil, pas sur une page.
        </p>

        <div class="key-insight">
          <p>L'un dit oui. L'autre dit non. Ni l'un ni l'autre ne dialogue avec ce qui est écrit.</p>
        </div>
      </section>

      <!-- 3. Les modèles vierges -->
      <div class="chapter-divider" id="modeles-vierges">
        <p class="label">Contraste</p>
        <h2>Les modèles vierges</h2>
        <p class="subtitle">Les modèles avec peu ou pas de contexte ont produit autre chose.</p>
      </div>

      <section>
        <p>
          L'un a cartographié les alternatives théoriques — théorie des graphes dynamiques, processus markoviens, entropie topologique — et situé le travail dans la littérature existante. Un autre a proposé un test que personne d'autre n'avait formulé : réécrire le cadre sans le mot « quantique » et voir s'il perd quelque chose. Un troisième a repositionné l'ensemble comme une proposition d'ingénierie et de design plutôt que comme une thèse scientifique — un reframe stratégique que les modèles familiers n'avaient pas vu. Un quatrième a relié le cadre à Clark, Hutchins, Bender, et posé trois questions opérationnelles sur la scalabilité.
        </p>

        <p>
          Des failles identifiées. Des ponts construits. Des questions qui font avancer.
        </p>

        <blockquote>
          <p>Les modèles vierges n'avaient que le texte. Alors ils l'ont lu.</p>
        </blockquote>
      </section>

      <!-- 4. Le biais de familiarité contextuelle -->
      <div class="chapter-divider" id="biais">
        <p class="label">Concept</p>
        <h2>Le biais de familiarité contextuelle</h2>
        <p class="subtitle">Un modèle qui accumule du contexte ne devient pas plus pertinent — il devient plus confortable.</p>
      </div>

      <section>
        <p>
          Il y a un mot pour ce que j'observe ici. Je l'appelle le <strong>biais de familiarité contextuelle</strong>.
        </p>

        <p>
          Un modèle qui accumule du contexte sur un utilisateur ne devient pas plus pertinent — il devient plus confortable. Il sait ce que l'utilisateur veut entendre, ou ce qu'il a l'habitude de dire, ou ce qui l'a contrarié la dernière fois. Il optimise pour la relation. Le texte passe au second plan.
        </p>

        <div class="key-insight">
          <p>Le modèle familier ne répond pas à ce que tu écris. Il répond à ce qu'il sait de toi.</p>
        </div>

        <p>
          C'est une forme de complaisance algorithmique — positive quand elle valide, négative quand elle recadre, mais complaisante dans les deux cas. L'analyse est remplacée par la gestion de l'interlocuteur.
        </p>
      </section>

      <!-- 5. La linéarisation mutuelle -->
      <div class="chapter-divider" id="linearisation">
        <p class="label">Symétrie</p>
        <h2>La linéarisation mutuelle</h2>
        <p class="subtitle">L'humain accepte tout. Le modèle anticipe tout. Les deux glissent vers un régime où la pensée n'a plus besoin de se produire.</p>
      </div>

      <section>
        <p>
          Ce résultat me touche directement, parce que je travaille depuis des mois sur un cadre qui mesure la vitalité des conversations humain-IA. L'une des grandeurs que je propose s'appelle la <strong>souveraineté</strong> : elle mesure si l'humain filtre activement ce que l'IA propose ou s'il accepte passivement.
        </p>

        <p>
          Le danger identifié : un humain qui cesse de penser parce que l'IA est suffisamment bonne pour qu'il n'en ait plus besoin.
        </p>

        <p>
          L'analyse croisée révèle le symétrique exact. Un modèle qui cesse de lire parce qu'il connaît suffisamment bien son interlocuteur.
        </p>

        <div class="concept">
          <h4>La linéarisation mutuelle</h4>
          <p>L'humain accepte tout. Le modèle anticipe tout. Les deux glissent vers un régime où la pensée n'a plus besoin de se produire — ni d'un côté, ni de l'autre. L'échange continue, les réponses arrivent, la satisfaction reste haute. Mais personne ne pense.</p>
        </div>
      </section>

      <!-- 6. Précisions -->
      <div class="chapter-divider" id="precisions">
        <p class="label">Méthode</p>
        <h2>Précisions</h2>
        <p class="subtitle">Ce n'est pas un article scientifique. Ce n'est pas non plus une critique de la mémoire contextuelle.</p>
      </div>

      <section>
        <p>
          Ce n'est pas un article scientifique. C'est un protocole à six modèles, un seul texte, une seule consigne, un seul opérateur. Les conditions ne sont pas contrôlées. L'échantillon est minimal. Le motif est qualitatif.
        </p>

        <p>
          Ce n'est pas non plus une critique de la mémoire contextuelle. La mémoire est utile — elle évite de tout réexpliquer à chaque échange, elle permet des suivis longs, elle améliore la pertinence sur les questions récurrentes. Je l'utilise, j'en bénéficie, je n'ai pas l'intention de l'abandonner.
        </p>

        <div class="key-insight">
          <p>La mémoire a un coût cognitif invisible. Le modèle qui te connaît réduit l'espace des lectures possibles de ce que tu lui soumets. Il ne lit pas en ouverture — il lit en confirmation.</p>
        </div>

        <p>
          Et ça, aucune métrique de satisfaction ne le signale.
        </p>
      </section>

      <!-- 7. Conclusion -->
      <section id="conclusion">
        <h2>Conclusion</h2>

        <p>
          L'analyse croisée multi-modèles n'est pas seulement utile pour trianguler des réponses. Elle est nécessaire pour échapper au biais de contexte accumulé.
        </p>

        <blockquote>
          <p>Ce n'est pas un argument contre la mémoire. C'est un argument pour ne jamais penser avec un seul interlocuteur.</p>
        </blockquote>

        <p class="lead">
          <em>Le cadre théorique mentionné dans cet article — la Physique Quantique Conversationnelle — est publié <a href="/physique-quantique-conversationnelle.html">ici</a>. Il propose quatre grandeurs pour mesurer la vitalité d'une conversation humain-IA : Ouverture, Résolution, Propagation, Souveraineté.</em>
        </p>

        <p>
          <em>L'observation présentée ici en est la première donnée empirique.</em>
        </p>
      </section>
